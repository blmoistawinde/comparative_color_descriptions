{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (4.0, 0.5) \n",
    "import matplotlib.patches as mpathes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load color descriptors and values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "color label to pytorch tensor, only mean vector of the samples are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdict = {}\n",
    "basedir = \"./xkcd_colordata/\"\n",
    "for cfile in os.listdir(basedir):\n",
    "    if cfile.endswith(\".train\") or cfile.endswith(\".dev\"):\n",
    "        cname = cfile[:cfile.find(\".\")]\n",
    "        with open(basedir+cfile,\"rb\") as f:\n",
    "            all_smaples = np.array(pickle.load(f)) / 255   # normalized to [0,1]\n",
    "            cdict[cname] = torch.FloatTensor(all_smaples.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2091, 0.3767, 0.7748])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdict[\"blue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single word label to multi-word descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2words = {}\n",
    "with open(\"words_to_labels.txt\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        words, label = line.strip().split(',')\n",
    "        label2words[label] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'light blue'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2words[\"lightblue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract triples like `('blueviolet', ('bluer',), 'violet')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_compara = dict(line.strip().split(\":\") for line in open(\"comparatives.txt\", encoding=\"utf-8\"))\n",
    "to_more_quanti = dict(line.strip().split(\":\") for line in open(\"quantifiers.txt\", encoding=\"utf-8\"))\n",
    "\n",
    "triples = []\n",
    "for label in cdict:\n",
    "    words = label2words[label].split()\n",
    "    if len(words) > 1:\n",
    "        quantifier, base = words[0], \"\".join(words[1:])\n",
    "        if base in cdict:\n",
    "            if quantifier in to_compara:        # uni-gram('lighter',)\n",
    "                triples.append((label, (to_compara[quantifier],), base))\n",
    "            elif quantifier in to_more_quanti:  # bigram('more','bluish')\n",
    "                triples.append((label, (\"more\", to_more_quanti[quantifier]), base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('acidgreen', ('more', 'acidic'), 'green'),\n",
       " ('blueviolet', ('bluer',), 'violet'),\n",
       " ('blueygreen', ('bluer',), 'green'),\n",
       " ('blueypurple', ('bluer',), 'purple'),\n",
       " ('bluishgreen', ('more', 'bluish'), 'green')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-extracted subset of https://www.kaggle.com/yekenot/fasttext-crawl-300d-2m\n",
    "with open(\"embeddings.pickle\",\"rb\") as f:\n",
    "    embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(603,30)\n",
    "        self.fc2 = nn.Linear(33,3)\n",
    "    def forward(self, emb1, emb2, source_color):\n",
    "        x1 = self.fc1(torch.cat([emb1, emb2, source_color]))\n",
    "        wg = self.fc2(torch.cat([x1,source_color]))\n",
    "        return wg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "my_loss = lambda source, target, wg: (1-cos(wg, target-source)) + mse(target, source+wg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0, loss:281.0379943847656\n",
      "step:50, loss:51.73341751098633\n",
      "step:100, loss:45.4268684387207\n",
      "step:150, loss:44.92023468017578\n",
      "step:200, loss:39.38445281982422\n",
      "step:250, loss:37.8366584777832\n",
      "step:300, loss:37.3712158203125\n",
      "step:350, loss:39.405025482177734\n",
      "step:400, loss:37.89094543457031\n",
      "step:450, loss:37.310089111328125\n",
      "step:500, loss:36.994022369384766\n",
      "step:550, loss:36.575469970703125\n",
      "step:600, loss:36.40888214111328\n",
      "step:650, loss:36.22138977050781\n",
      "step:700, loss:36.5299072265625\n",
      "step:750, loss:37.493202209472656\n"
     ]
    }
   ],
   "source": [
    "epoches = 800\n",
    "net = ColorNet()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "for i in range(epoches):\n",
    "    loss = 0\n",
    "    for target_str, comp_words, source_str in triples:\n",
    "        if len(comp_words) == 1:\n",
    "            emb1, emb2 = torch.zeros(300,), torch.from_numpy(embeddings[comp_words[0]])\n",
    "        else:\n",
    "            emb1, emb2 = torch.from_numpy(embeddings[comp_words[0]]), torch.from_numpy(embeddings[comp_words[1]])\n",
    "        emb1, emb2 = torch.FloatTensor(emb1), torch.FloatTensor(emb2)\n",
    "        target, source = cdict[target_str], cdict[source_str]\n",
    "        wg = net(emb1, emb2, source)\n",
    "        loss += my_loss(source, target, wg)\n",
    "    if i % 50 == 0:\n",
    "        print(f\"step:{i}, loss:{loss.detach().numpy()}\")\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compara_direction(compara, source_str):\n",
    "    '''\n",
    "    compara: comparative color descriptions like `lighter`\n",
    "    source_str: source color like `blue`\n",
    "    '''\n",
    "    comp_words = compara.split()\n",
    "    if len(comp_words) == 1:\n",
    "        emb1, emb2 = torch.zeros(300,), torch.from_numpy(embeddings[comp_words[0]])\n",
    "    else:\n",
    "        emb1, emb2 = torch.from_numpy(embeddings[comp_words[0]]), torch.from_numpy(embeddings[comp_words[1]])\n",
    "    emb1, emb2 = torch.FloatTensor(emb1), torch.FloatTensor(emb2)\n",
    "    source = cdict[source_str]\n",
    "    wg = net(emb1, emb2, source)\n",
    "    return wg.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08290963, 0.15813543, 0.07795188], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_compara_direction(\"lighter\",\"purple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07854243, -0.04148286, -0.10587399], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_compara_direction(\"darker\",\"purple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12483665, 0.12858301, 0.06017698], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_compara_direction(\"lighter\",\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09904111,  0.07201194,  0.15063742], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_compara_direction(\"more blueish\",\"purple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05477055,  0.0819556 ,  0.24324164], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_compara_direction(\"more blueish\",\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_color_change(compara, source_str, strength=1, save_path=None):\n",
    "    '''\n",
    "    compara: comparative color descriptions like `lighter`\n",
    "    source_str: source color like `blue`\n",
    "    strength: the ratio of exaggerating the effect to make it more perceivable\n",
    "    '''\n",
    "    ax = plt.gca()\n",
    "    source = cdict[source_str].detach().numpy()\n",
    "    direction = get_compara_direction(compara, source_str)\n",
    "    ax = plt.gca()\n",
    "    N = 100\n",
    "    width, height = 1, 1\n",
    "    for x in np.linspace(0, width, N):\n",
    "        ax.add_patch(mpathes.Rectangle([x,0],width/N,height,color=np.clip(source+direction*x*strength,0,1)))\n",
    "    plt.axis('off')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAA+CAYAAADAgBUyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAAb9JREFUeJzt1kFO21AUQNH7o+6BadUutoPuoetr\nNkEYQHAcO4kJQpHac0aA7e9n++uKcTgcAv5vu0cPADyeEABCAAgBkBAACQGQEAAJAZAQAAkBUH37\nikV/fP/5t3qqnsdoVz03xu7193GMz/Hn6VjLc6br2q1cM1vn/djKOtN1G9ZpbJh5vs5s5vn9b8x8\n5f2sz7EfY3R8v2fHjuvtazpnyxwX1rnjec6/aRvutfa9z7/pcp0L663fa3Vvre6FT69z/95a3Qvr\n33s+x763c/78/jW6w5eEoNehe99Mbw978tDTz8djLc+Zrmu3ds3p36brrt1rwzqNDTPP15nNfHb/\n6zNfeT+X53havp+z9U7O2TLHlXU++Dzn37QN91r73otjt+/14b21vhc+u879e+viXlh+7+Uc0zl3\n2t0+BfjXCQEgBIAQAAkBkBAACQGQEAAJAZAQAAkBkBAACQGQEAAJAZAQAAkBkBAACQGQEAAJAZAQ\nAAkBkBAACQFQjcPh8OgZgAfzHwEgBIAQAAkBkBAACQGQEAAJAZAQAAkBkBAACQGQEAAJAZAQAAkB\nkBAACQGQEAAJAZAQAAkBkBAA1Qsps26KzVSXQgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x36 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_color_change(\"lighter\", \"black\", strength=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAA+CAYAAADAgBUyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAAX1JREFUeJzt3cFNG0EYgNHPFj24E/pJF+mPA02k\ngNBEJgdjbK13FgFxkOC9ywrNaP9Zr/WJAxK7MUbA97b/7AMAn08IACEAhABICICEAEgIgIQASAiA\nhACo7m5x0/sfj7+rQ40/1b7xfK3TtdPaWK6df37Zc1q72nu5Z7E2urjPdMblrNmM+X3G9exmM8bl\nmWczVu6zMuPpef2wMeOp45+OH7ZmjPnnsXLWxYy1Z17MWH/m+fse87UPfW/O59l6T9fvez5j63uz\n9Rm+5TzX73tjxsv7/vXwc9c73CQEHb+AdX7I5XVrzZ7ttcM/2vM/zmrPx/e89X2/y/71LcBXJwSA\nEABCACQEQEIAJARAQgAkBEBCACQEQEIAJARAQgAkBEBCACQEQEIAJARAQgAkBEBCACQEQEIAJARA\ntRvHf5UEfGN+IwCEABACICEAEgIgIQASAiAhABICICEAEgIgIQASAiAhABICICEAEgIgIQASAiAh\nABICICEAEgKg+gsl6uKoq32vQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x36 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_color_change(\"darker\", \"blue\",2,'./imgs/darker_blue.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAA+CAYAAADAgBUyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAAX9JREFUeJzt2kFu2lAUQNFr1D2wkm6pq6k67iq6\nrmYT+R0EKmpjQ2gQqD1ngoIf1sO2rhhkGmME/N92j14AeDwhAIQAEAIgIQASAiAhABICICEAEgKg\n+nSPk377/ONntR/12ltsXpva1Ti8Vodj4/ex+WyNxvy9G2eO740rZk52vTizmG3j+5zMrO5x5jyL\nPV4O/xS+X35uXDFz4dpdv8eNM6v3YuPa/cX1PTl28tnW9jg/8+ce2zPH8yyf9e19Vq/dy5jm93L9\nmfj6/cvUDe4Sgt6WruMvjmn2eu7Y8u/nn5nPbn/+o2b2HzTzLN/nfjPP+ty8Z+a99/tGu8sjwL9O\nCAAhAIQASAiAhABICICEAEgIgIQASAiAhABICICEAEgIgIQASAiAhABICICEAEgIgIQASAiAhABI\nCIBqGmM8egfgwfwiAIQAEAIgIQASAiAhABICICEAEgIgIQASAiAhABICICEAEgIgIQASAiAhABIC\nICEAEgIgIQASAqD6BaSVm9pphQSyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x36 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_color_change(\"more electric\", \"purple\",1 ,'./imgs/more_electric_purple.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
